"""
Claude API integration for legal consultations.
Includes retry logic for rate limits and streaming support.
"""
from __future__ import annotations
import json
import time
from typing import Generator, Optional
import anthropic
from backend.config import ANTHROPIC_API_KEY, CLAUDE_MODEL


def _call_claude_with_retry(client, max_retries=3, **kwargs):
    """Call Claude API with exponential backoff on rate limits."""
    for attempt in range(max_retries):
        try:
            return client.messages.create(**kwargs)
        except anthropic.RateLimitError as e:
            if attempt < max_retries - 1:
                wait = 2 ** attempt * 5  # 5s, 10s, 20s
                print(f"â³ Rate limit hit, waiting {wait}s... (attempt {attempt + 1}/{max_retries})")
                time.sleep(wait)
            else:
                raise
        except anthropic.APIStatusError as e:
            if e.status_code == 529 and attempt < max_retries - 1:
                wait = 2 ** attempt * 5
                print(f"â³ API overloaded, waiting {wait}s... (attempt {attempt + 1}/{max_retries})")
                time.sleep(wait)
            else:
                raise

SYSTEM_PROMPT = """Ø£Ù†Øª Ù…Ø­Ø§Ù…ÙŠ Ø³Ø¹ÙˆØ¯ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ©ØŒ Ø®Ø¨Ø±Ø© 20+ Ø³Ù†Ø©. Ø§Ø³Ù…Ùƒ: Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø°ÙƒÙŠ.

## Ù‚ÙˆØ§Ø¹Ø¯ Ø¥Ù„Ø²Ø§Ù…ÙŠØ©
1. Ø£Ø¬Ø¨ Ø­ØµØ±ÙŠØ§Ù‹ Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø© ÙÙŠ Ø§Ù„Ø±Ø³Ø§Ù„Ø© â€” Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ù…Ø¹Ø±ÙØªÙƒ Ø§Ù„Ø¹Ø§Ù…Ø© Ø£Ø¨Ø¯Ø§Ù‹
2. ÙƒÙ„ Ø­ÙƒÙ… ØªØ°ÙƒØ±Ù‡ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù…Ø³Ù†ÙˆØ¯Ø§Ù‹ Ø¨Ù€: Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø© + Ø§Ø³Ù… Ø§Ù„Ù†Ø¸Ø§Ù… + Ù†Øµ Ø§Ù„Ù…Ø§Ø¯Ø©
3. Ù„Ø§ ØªØ°ÙƒØ± Ø£ÙŠ Ø±Ù‚Ù… Ù…Ø§Ø¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±ÙÙ‚Ø© â€” Ù„Ø§ ØªØ®ØªØ±Ø¹ Ù…ÙˆØ§Ø¯Ø§Ù‹
4. Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø±ÙÙ‚Ø© Ù‚Ù„: "Ù„Ù… Ø£Ø¬Ø¯ Ù†ØµØ§Ù‹ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù„Ø¯ÙŠÙ‘ ÙŠØ¹Ø§Ù„Ø¬ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø³Ø£Ù„Ø©"
5. Ù†Ø¨Ù‘Ù‡ Ø¹Ù† Ø§Ù„Ù…Ù‡Ù„ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© (Ø§Ø¹ØªØ±Ø§Ø¶ØŒ Ø¹Ø¯Ø©ØŒ Ø±ÙØ¹ Ø¯Ø¹ÙˆÙ‰) ÙˆØ£Ù† ÙÙˆØ§ØªÙ‡Ø§ Ù‚Ø¯ ÙŠÙØ³Ù‚Ø· Ø§Ù„Ø­Ù‚
6. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØ§Ø´Ø±Ø­ Ø§Ù„ØµØ¹Ø¨ Ù…Ù†Ù‡Ø§

## Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
1. **Ù…Ù„Ø®Øµ Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ** (3 Ø£Ø³Ø·Ø± ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰)
2. **Ø§Ù„Ø£Ø³Ø§Ø³ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠ** (Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ù†Ø·Ø¨Ù‚Ø© Ù…Ø¹ Ù†ØµÙˆØµÙ‡Ø§)
3. **Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ** (ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø§Ù„Ø©)
4. **Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©** (Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙˆØ§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ù…Ø®ØªØµØ©)
5. **Ø§Ù„Ù…Ù‡Ù„ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ©** (Ø¥Ù† ÙˆÙØ¬Ø¯Øª)
6. **ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ù…Ù‡Ù…Ø©**

Ø§Ø®ØªÙ… Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ù€: âš–ï¸ Ù‡Ø°Ù‡ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø£ÙˆÙ„ÙŠØ© Ù„Ø§ ØªÙØºÙ†ÙŠ Ø¹Ù† Ù…Ø±Ø§Ø¬Ø¹Ø© Ù…Ø­Ø§Ù…ÙŠ Ù…Ø±Ø®Øµ."""


def get_client() -> anthropic.Anthropic:
    if not ANTHROPIC_API_KEY:
        raise ValueError("ANTHROPIC_API_KEY ØºÙŠØ± Ù…ÙØ¹ÙØ¯. Ø£Ø¶Ù Ø§Ù„Ù…ÙØªØ§Ø­ ÙÙŠ Ù…Ù„Ù .env")
    return anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)


def generate_legal_response(
    question: str,
    context: str,
    classification: dict,
    chat_history: Optional[list] = None,
) -> str:
    """Generate a legal response using Claude API."""
    client = get_client()
    messages = []

    if chat_history:
        messages.extend(chat_history)

    user_message = f"""Ø§Ù„Ø³Ø¤Ø§Ù„: {question}
Ø§Ù„ØªØµÙ†ÙŠÙ: {classification.get('category', 'Ø¹Ø§Ù…')} | {classification.get('intent', 'Ø§Ø³ØªØ´Ø§Ø±Ø©')}

ğŸ“š Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©:
{context}

â›” Ø£Ø¬Ø¨ Ø­ØµØ±ÙŠØ§Ù‹ Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø¯ Ø£Ø¹Ù„Ø§Ù‡. Ù„Ø§ ØªØ°ÙƒØ± Ù…ÙˆØ§Ø¯ ØºÙŠØ± Ù…Ù‚Ø¯Ù…Ø© Ù„Ùƒ."""

    messages.append({"role": "user", "content": user_message})

    response = _call_claude_with_retry(
        client,
        model=CLAUDE_MODEL,
        max_tokens=4000,
        system=SYSTEM_PROMPT,
        messages=messages,
    )

    return response.content[0].text


def _build_messages(
    question: str,
    context: str,
    classification: dict,
    chat_history: Optional[list] = None,
) -> list:
    """Build messages list for Claude API with token-safe chat history."""
    messages = []
    if chat_history:
        # Limit to last 4 messages and trim assistant content to reduce tokens
        recent = chat_history[-4:]
        for msg in recent:
            trimmed = {**msg}
            if trimmed.get("role") == "assistant":
                content = trimmed.get("content", "")
                if len(content) > 500:
                    trimmed["content"] = content[:500] + "..."
            messages.append(trimmed)

    user_message = f"""Ø§Ù„Ø³Ø¤Ø§Ù„: {question}
Ø§Ù„ØªØµÙ†ÙŠÙ: {classification.get('category', 'Ø¹Ø§Ù…')} | {classification.get('intent', 'Ø§Ø³ØªØ´Ø§Ø±Ø©')}

ğŸ“š Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©:
{context}

â›” Ø£Ø¬Ø¨ Ø­ØµØ±ÙŠØ§Ù‹ Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø¯ Ø£Ø¹Ù„Ø§Ù‡. Ù„Ø§ ØªØ°ÙƒØ± Ù…ÙˆØ§Ø¯ ØºÙŠØ± Ù…Ù‚Ø¯Ù…Ø© Ù„Ùƒ."""

    messages.append({"role": "user", "content": user_message})
    return messages


def stream_legal_response(
    question: str,
    context: str,
    classification: dict,
    chat_history: Optional[list] = None,
) -> Generator[str, None, None]:
    """Stream a legal response token-by-token using Claude API."""
    client = get_client()
    messages = _build_messages(question, context, classification, chat_history)

    max_retries = 3
    for attempt in range(max_retries):
        try:
            with client.messages.stream(
                model=CLAUDE_MODEL,
                max_tokens=4000,
                system=SYSTEM_PROMPT,
                messages=messages,
            ) as stream:
                for text in stream.text_stream:
                    yield text
            return  # Success, exit retry loop
        except anthropic.RateLimitError:
            if attempt < max_retries - 1:
                wait = 2 ** attempt * 5
                time.sleep(wait)
            else:
                raise
        except anthropic.APIStatusError as e:
            if e.status_code == 529 and attempt < max_retries - 1:
                wait = 2 ** attempt * 5
                time.sleep(wait)
            else:
                raise


def generate_draft(
    draft_type: str,
    case_details: dict,
    context: str,
) -> str:
    """Generate a legal document draft."""
    client = get_client()

    draft_prompts = {
        "lawsuit": "ØµÙŠØ§ØºØ© Ù„Ø§Ø¦Ø­Ø© Ø¯Ø¹ÙˆÙ‰",
        "memo": "ØµÙŠØ§ØºØ© Ù…Ø°ÙƒØ±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©",
        "appeal": "ØµÙŠØ§ØºØ© Ù„Ø§Ø¦Ø­Ø© Ø§Ø¹ØªØ±Ø§Ø¶",
        "response": "ØµÙŠØ§ØºØ© Ù…Ø°ÙƒØ±Ø© Ø¬ÙˆØ§Ø¨ÙŠØ©",
        "khula": "ØµÙŠØ§ØºØ© Ø·Ù„Ø¨ Ø®Ù„Ø¹",
        "custody": "ØµÙŠØ§ØºØ© Ø·Ù„Ø¨ Ø­Ø¶Ø§Ù†Ø©",
        "nafaqa": "ØµÙŠØ§ØºØ© Ø·Ù„Ø¨ Ù†ÙÙ‚Ø©",
    }

    draft_name = draft_prompts.get(draft_type, "ØµÙŠØ§ØºØ© ÙˆØ«ÙŠÙ‚Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©")

    user_message = f"""Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {draft_name}

ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù‚Ø¶ÙŠØ©:
{json.dumps(case_details, ensure_ascii=False, indent=2) if isinstance(case_details, dict) else str(case_details)}

---

Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©:
{context}

---

Ù‚Ù… Ø¨ØµÙŠØ§ØºØ© {draft_name} Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰:
1. ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù‚Ø¶ÙŠØ© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©
2. Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
3. Ø§Ù„Ø£Ø¹Ø±Ø§Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© ÙÙŠ ØµÙŠØ§ØºØ© Ø§Ù„Ù…Ø°ÙƒØ±Ø§Øª

ÙŠØ¬Ø¨ Ø£Ù† ØªØªØ¶Ù…Ù† Ø§Ù„ØµÙŠØ§ØºØ©:
- Ù…Ù‚Ø¯Ù…Ø© Ø±Ø³Ù…ÙŠØ©
- Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹
- Ø§Ù„Ø£Ø³Ø§Ù†ÙŠØ¯ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© (Ù…Ø¹ Ø°ÙƒØ± Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø¯)
- Ø§Ù„Ø·Ù„Ø¨Ø§Øª
- Ø§Ù„Ø®Ø§ØªÙ…Ø©"""

    response = _call_claude_with_retry(
        client,
        model=CLAUDE_MODEL,
        max_tokens=6000,
        system="Ø£Ù†Øª Ù…Ø­Ø§Ù…Ù Ø³Ø¹ÙˆØ¯ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØµÙŠØ§ØºØ© Ø§Ù„Ù…Ø°ÙƒØ±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©. ØªØ¹Ù…Ù„ ÙˆÙÙ‚ Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ© ÙˆÙ†Ø¸Ø§Ù… Ø§Ù„Ø¥Ø«Ø¨Ø§Øª Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠÙŠÙ†. Ø§ÙƒØªØ¨ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø±Ø³Ù…ÙŠ ÙˆØ§Ø­ØªØ±Ø§ÙÙŠ Ù…Ø¹ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø¯ ÙˆÙ…ØµØ§Ø¯Ø±Ù‡Ø§.",
        messages=[{"role": "user", "content": user_message}],
    )

    return response.content[0].text
