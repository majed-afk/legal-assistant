"""
Full RAG pipeline: from user question to relevant legal context.
Hybrid search: combines semantic (vector) search with keyword-based topic filtering.
This compensates for the multilingual model's weaker Arabic legal term understanding.
"""
from __future__ import annotations
from backend.rag.embeddings import embed_query_list
from backend.rag.vector_store import search
from backend.rag.classifier import classify_query

# Cache for RAG results (question -> context).
_rag_cache: dict[str, dict] = {}
_RAG_CACHE_MAX = 32

# Legal terms â†’ exact ChromaDB topic names for precise filtering.
# Longest-match-first: put compound terms before single words.
# Covers all 62 topics across 3 laws.
LEGAL_TERM_MAP = {
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ© (27 topic)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ø§Ù„Ø®Ø·Ø¨Ø©
    "Ø®Ø·Ø¨Ø©": "Ø§Ù„Ø®Ø·Ø¨Ø©", "Ø®Ø§Ø·Ø¨": "Ø§Ù„Ø®Ø·Ø¨Ø©", "Ù…Ø®Ø·ÙˆØ¨Ø©": "Ø§Ù„Ø®Ø·Ø¨Ø©", "Ø¹Ø¯ÙˆÙ„ Ø¹Ù† Ø§Ù„Ø®Ø·Ø¨Ø©": "Ø§Ù„Ø®Ø·Ø¨Ø©",
    # Ø¹Ù‚Ø¯ Ø§Ù„Ø²ÙˆØ§Ø¬ (Ø£Ø­ÙƒØ§Ù… Ø¹Ø§Ù…Ø©)
    "Ø¹Ù‚Ø¯ Ø§Ù„Ø²ÙˆØ§Ø¬": "Ø¹Ù‚Ø¯ Ø§Ù„Ø²ÙˆØ§Ø¬", "Ø¹Ù‚Ø¯ Ø§Ù„Ù†ÙƒØ§Ø­": "Ø¹Ù‚Ø¯ Ø§Ù„Ø²ÙˆØ§Ø¬", "ØªÙˆØ«ÙŠÙ‚ Ø§Ù„Ø²ÙˆØ§Ø¬": "Ø¹Ù‚Ø¯ Ø§Ù„Ø²ÙˆØ§Ø¬",
    # Ø£Ø±ÙƒØ§Ù† Ø§Ù„Ø²ÙˆØ§Ø¬
    "Ø£Ø±ÙƒØ§Ù† Ø§Ù„Ø²ÙˆØ§Ø¬": "Ø£Ø±ÙƒØ§Ù† Ø§Ù„Ø²ÙˆØ§Ø¬", "Ø£Ø±ÙƒØ§Ù† Ø¹Ù‚Ø¯ Ø§Ù„Ø²ÙˆØ§Ø¬": "Ø£Ø±ÙƒØ§Ù† Ø§Ù„Ø²ÙˆØ§Ø¬", "Ø£Ø±ÙƒØ§Ù† Ø§Ù„Ù†ÙƒØ§Ø­": "Ø£Ø±ÙƒØ§Ù† Ø§Ù„Ø²ÙˆØ§Ø¬",
    "Ø£Ø±ÙƒØ§Ù†": "Ø£Ø±ÙƒØ§Ù† Ø§Ù„Ø²ÙˆØ§Ø¬", "Ø¥ÙŠØ¬Ø§Ø¨ ÙˆÙ‚Ø¨ÙˆÙ„": "Ø£Ø±ÙƒØ§Ù† Ø§Ù„Ø²ÙˆØ§Ø¬", "Ø±ÙƒÙ†": "Ø£Ø±ÙƒØ§Ù† Ø§Ù„Ø²ÙˆØ§Ø¬",
    # Ø´Ø±ÙˆØ· Ø§Ù„Ø²ÙˆØ§Ø¬ â€” compound terms BEFORE generic "Ø´Ø±ÙˆØ·"
    "Ø´Ø±ÙˆØ· Ø¹Ù‚Ø¯ Ø§Ù„Ø²ÙˆØ§Ø¬": "Ø´Ø±ÙˆØ· Ø§Ù„Ø²ÙˆØ§Ø¬", "Ø´Ø±ÙˆØ· Ø§Ù„Ø²ÙˆØ§Ø¬": "Ø´Ø±ÙˆØ· Ø§Ù„Ø²ÙˆØ§Ø¬", "Ø´Ø±ÙˆØ· Ø§Ù„Ù†ÙƒØ§Ø­": "Ø´Ø±ÙˆØ· Ø§Ù„Ø²ÙˆØ§Ø¬",
    "Ø´Ø±ÙˆØ· ÙÙŠ Ø§Ù„Ø¹Ù‚Ø¯": "Ø´Ø±ÙˆØ· Ø§Ù„Ø²ÙˆØ§Ø¬",
    # Ø§Ù„ÙˆÙ„Ø§ÙŠØ© ÙÙŠ Ø§Ù„Ø²ÙˆØ§Ø¬
    "ÙˆÙ„Ø§ÙŠØ© Ø§Ù„Ø²ÙˆØ§Ø¬": "Ø§Ù„ÙˆÙ„Ø§ÙŠØ© ÙÙŠ Ø§Ù„Ø²ÙˆØ§Ø¬", "ÙˆÙ„ÙŠ ÙÙŠ Ø§Ù„Ø²ÙˆØ§Ø¬": "Ø§Ù„ÙˆÙ„Ø§ÙŠØ© ÙÙŠ Ø§Ù„Ø²ÙˆØ§Ø¬",
    "ÙˆÙ„ÙŠ": "Ø§Ù„ÙˆÙ„Ø§ÙŠØ© ÙÙŠ Ø§Ù„Ø²ÙˆØ§Ø¬", "Ø¹Ø¶Ù„": "Ø§Ù„ÙˆÙ„Ø§ÙŠØ© ÙÙŠ Ø§Ù„Ø²ÙˆØ§Ø¬",
    # Ø§Ù„Ù…Ø­Ø±Ù…Ø§Øª
    "Ù…Ø­Ø±Ù…Ø§Øª": "Ø§Ù„Ù…Ø­Ø±Ù…Ø§Øª", "Ù…Ø­Ø±Ù…": "Ø§Ù„Ù…Ø­Ø±Ù…Ø§Øª", "Ù…Ø­Ø±Ù…Ø§Øª Ø§Ù„Ù†ÙƒØ§Ø­": "Ø§Ù„Ù…Ø­Ø±Ù…Ø§Øª",
    "Ø±Ø¶Ø§Ø¹": "Ø§Ù„Ù…Ø­Ø±Ù…Ø§Øª", "Ù…ØµØ§Ù‡Ø±Ø©": "Ø§Ù„Ù…Ø­Ø±Ù…Ø§Øª", "Ø¬Ù…Ø¹ Ø¨ÙŠÙ†": "Ø§Ù„Ù…Ø­Ø±Ù…Ø§Øª",
    # Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø²ÙˆØ§Ø¬
    "Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø²ÙˆØ§Ø¬": "Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø²ÙˆØ§Ø¬", "Ø²ÙˆØ§Ø¬ Ø§Ù„Ù…Ø³ÙŠØ§Ø±": "Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø²ÙˆØ§Ø¬", "ØªØ¹Ø¯Ø¯ Ø§Ù„Ø²ÙˆØ¬Ø§Øª": "Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø²ÙˆØ§Ø¬",
    "ØªØ¹Ø¯Ø¯": "Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø²ÙˆØ§Ø¬",
    # Ø§Ù„Ø²ÙˆØ§Ø¬ Ø§Ù„Ø¨Ø§Ø·Ù„ ÙˆØ§Ù„ÙØ§Ø³Ø¯
    "Ø§Ù„Ø²ÙˆØ§Ø¬ Ø§Ù„Ø¨Ø§Ø·Ù„": "Ø§Ù„Ø²ÙˆØ§Ø¬ Ø§Ù„Ø¨Ø§Ø·Ù„ ÙˆØ§Ù„ÙØ§Ø³Ø¯", "Ø§Ù„Ø²ÙˆØ§Ø¬ Ø§Ù„ÙØ§Ø³Ø¯": "Ø§Ù„Ø²ÙˆØ§Ø¬ Ø§Ù„Ø¨Ø§Ø·Ù„ ÙˆØ§Ù„ÙØ§Ø³Ø¯",
    "Ø²ÙˆØ§Ø¬ Ø¨Ø§Ø·Ù„": "Ø§Ù„Ø²ÙˆØ§Ø¬ Ø§Ù„Ø¨Ø§Ø·Ù„ ÙˆØ§Ù„ÙØ§Ø³Ø¯", "Ø²ÙˆØ§Ø¬ ÙØ§Ø³Ø¯": "Ø§Ù„Ø²ÙˆØ§Ø¬ Ø§Ù„Ø¨Ø§Ø·Ù„ ÙˆØ§Ù„ÙØ§Ø³Ø¯",
    "Ø¨Ø·Ù„Ø§Ù† Ø§Ù„Ø²ÙˆØ§Ø¬": "Ø§Ù„Ø²ÙˆØ§Ø¬ Ø§Ù„Ø¨Ø§Ø·Ù„ ÙˆØ§Ù„ÙØ§Ø³Ø¯", "ÙØ³Ø§Ø¯ Ø§Ù„Ø²ÙˆØ§Ø¬": "Ø§Ù„Ø²ÙˆØ§Ø¬ Ø§Ù„Ø¨Ø§Ø·Ù„ ÙˆØ§Ù„ÙØ§Ø³Ø¯",
    "Ø¨Ø§Ø·Ù„": "Ø§Ù„Ø²ÙˆØ§Ø¬ Ø§Ù„Ø¨Ø§Ø·Ù„ ÙˆØ§Ù„ÙØ§Ø³Ø¯",
    # Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø²ÙˆØ¬ÙŠÙ†
    "Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø²ÙˆØ¬ÙŠÙ†": "Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø²ÙˆØ¬ÙŠÙ†", "Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø²ÙˆØ¬": "Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø²ÙˆØ¬ÙŠÙ†", "Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø²ÙˆØ¬Ø©": "Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø²ÙˆØ¬ÙŠÙ†",
    # Ø§Ù„Ù…Ù‡Ø±
    "Ù…Ù‡Ø±": "Ø§Ù„Ù…Ù‡Ø±", "ØµØ¯Ø§Ù‚": "Ø§Ù„Ù…Ù‡Ø±", "Ù…Ù‡Ø± Ø§Ù„Ù…Ø«Ù„": "Ø§Ù„Ù…Ù‡Ø±", "Ù…Ù‚Ø¯Ù… Ø§Ù„Ù…Ù‡Ø±": "Ø§Ù„Ù…Ù‡Ø±", "Ù…Ø¤Ø®Ø± Ø§Ù„Ù…Ù‡Ø±": "Ø§Ù„Ù…Ù‡Ø±",
    # Ø²ÙˆØ§Ø¬ (Ø¹Ø§Ù… â€” fallback Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø±ÙƒØ¨Ø§Øª)
    "Ø²ÙˆØ§Ø¬": "Ø¹Ù‚Ø¯ Ø§Ù„Ø²ÙˆØ§Ø¬", "Ù†ÙƒØ§Ø­": "Ø¹Ù‚Ø¯ Ø§Ù„Ø²ÙˆØ§Ø¬",
    # Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„ÙØ±Ù‚Ø©
    "ÙØ±Ù‚Ø©": "Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„ÙØ±Ù‚Ø©", "Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„ÙØ±Ù‚Ø©": "Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„ÙØ±Ù‚Ø©",
    # Ø§Ù„Ø·Ù„Ø§Ù‚
    "Ø·Ù„Ø§Ù‚": "Ø§Ù„Ø·Ù„Ø§Ù‚", "ØªØ·Ù„ÙŠÙ‚": "Ø§Ù„Ø·Ù„Ø§Ù‚", "Ø±Ø¬Ø¹ÙŠ": "Ø§Ù„Ø·Ù„Ø§Ù‚", "Ø¨Ø§Ø¦Ù†": "Ø§Ù„Ø·Ù„Ø§Ù‚",
    "Ù…Ø±Ø§Ø¬Ø¹Ø©": "Ø§Ù„Ø·Ù„Ø§Ù‚", "Ø·Ù„Ù‚Ø©": "Ø§Ù„Ø·Ù„Ø§Ù‚", "ÙŠÙ…ÙŠÙ† Ø§Ù„Ø·Ù„Ø§Ù‚": "Ø§Ù„Ø·Ù„Ø§Ù‚",
    # Ø§Ù„Ø®Ù„Ø¹
    "Ø®Ù„Ø¹": "Ø§Ù„Ø®Ù„Ø¹", "Ù…Ø®Ø§Ù„Ø¹Ø©": "Ø§Ù„Ø®Ù„Ø¹", "Ø§ÙØªØ¯Ø§Ø¡": "Ø§Ù„Ø®Ù„Ø¹", "Ø¹ÙˆØ¶ Ø§Ù„Ø®Ù„Ø¹": "Ø§Ù„Ø®Ù„Ø¹",
    # ÙØ³Ø® Ø§Ù„Ù†ÙƒØ§Ø­
    "ÙØ³Ø® Ø§Ù„Ù†ÙƒØ§Ø­": "ÙØ³Ø® Ø§Ù„Ù†ÙƒØ§Ø­", "ÙØ³Ø®": "ÙØ³Ø® Ø§Ù„Ù†ÙƒØ§Ø­", "ØªÙØ±ÙŠÙ‚ Ù„Ù„Ø¶Ø±Ø±": "ÙØ³Ø® Ø§Ù„Ù†ÙƒØ§Ø­",
    "ØªÙØ±ÙŠÙ‚": "ÙØ³Ø® Ø§Ù„Ù†ÙƒØ§Ø­", "Ø´Ù‚Ø§Ù‚": "ÙØ³Ø® Ø§Ù„Ù†ÙƒØ§Ø­", "Ø¶Ø±Ø±": "ÙØ³Ø® Ø§Ù„Ù†ÙƒØ§Ø­", "Ø¥ÙŠÙ„Ø§Ø¡": "ÙØ³Ø® Ø§Ù„Ù†ÙƒØ§Ø­",
    # Ø§Ù„Ø¹Ø¯Ø©
    "Ø¹Ø¯Ø©": "Ø§Ù„Ø¹Ø¯Ø©", "Ø¹Ø¯Ø© Ø§Ù„ÙˆÙØ§Ø©": "Ø§Ù„Ø¹Ø¯Ø©", "Ø¹Ø¯Ø© Ø§Ù„Ø·Ù„Ø§Ù‚": "Ø§Ù„Ø¹Ø¯Ø©", "Ø¹Ø¯Ø© Ø§Ù„Ø­Ø§Ù…Ù„": "Ø§Ù„Ø¹Ø¯Ø©",
    # Ø§Ù„Ø­Ø¶Ø§Ù†Ø©
    "Ø­Ø¶Ø§Ù†Ø©": "Ø§Ù„Ø­Ø¶Ø§Ù†Ø©", "Ù…Ø­Ø¶ÙˆÙ†": "Ø§Ù„Ø­Ø¶Ø§Ù†Ø©", "Ø­Ø§Ø¶Ù†": "Ø§Ù„Ø­Ø¶Ø§Ù†Ø©", "Ø³Ù† Ø§Ù„Ø­Ø¶Ø§Ù†Ø©": "Ø§Ù„Ø­Ø¶Ø§Ù†Ø©",
    # Ø§Ù„Ù†ÙÙ‚Ø©
    "Ù†ÙÙ‚Ø©": "Ø§Ù„Ù†ÙÙ‚Ø©", "Ù†ÙÙ‚Ø© Ø§Ù„Ø²ÙˆØ¬Ø©": "Ø§Ù„Ù†ÙÙ‚Ø©", "Ø¥Ù†ÙØ§Ù‚": "Ø§Ù„Ù†ÙÙ‚Ø©",
    "Ù†ÙÙ‚Ø© Ø§Ù„Ø£Ù‚Ø§Ø±Ø¨": "Ù†ÙÙ‚Ø© Ø§Ù„Ø£Ù‚Ø§Ø±Ø¨", "Ù†ÙÙ‚Ø© Ø§Ù„Ø£ÙˆÙ„Ø§Ø¯": "Ù†ÙÙ‚Ø© Ø§Ù„Ø£Ù‚Ø§Ø±Ø¨", "Ù†ÙÙ‚Ø© Ø§Ù„ÙˆØ§Ù„Ø¯ÙŠÙ†": "Ù†ÙÙ‚Ø© Ø§Ù„Ø£Ù‚Ø§Ø±Ø¨",
    # Ø§Ù„Ù†Ø³Ø¨
    "Ù†Ø³Ø¨": "Ø§Ù„Ù†Ø³Ø¨", "Ø¥Ø«Ø¨Ø§Øª Ø§Ù„Ù†Ø³Ø¨": "Ø§Ù„Ù†Ø³Ø¨", "Ù†ÙÙŠ Ø§Ù„Ù†Ø³Ø¨": "Ø§Ù„Ù†Ø³Ø¨", "Ù„Ø¹Ø§Ù†": "Ø§Ù„Ù†Ø³Ø¨",
    # Ø§Ù„ÙˆØµÙŠØ©
    "ÙˆØµÙŠØ©": "Ø§Ù„ÙˆØµÙŠØ©", "Ù…ÙˆØµÙŠ": "Ø§Ù„ÙˆØµÙŠØ©", "Ù…ÙˆØµÙ‰": "Ø§Ù„ÙˆØµÙŠØ©", "Ø«Ù„Ø« Ø§Ù„ØªØ±ÙƒØ©": "Ø§Ù„ÙˆØµÙŠØ©",
    # Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ø¥Ø±Ø«
    "Ø¥Ø±Ø«": "Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ø¥Ø±Ø«", "Ù…ÙŠØ±Ø§Ø«": "Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ø¥Ø±Ø«", "ØªØ±ÙƒØ©": "Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ø¥Ø±Ø«", "ÙˆØ±Ø«Ø©": "Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ø¥Ø±Ø«",
    # Ø§Ù„Ø¥Ø±Ø« Ø¨Ø§Ù„ÙØ±Ø¶ / Ø§Ù„ØªØ¹ØµÙŠØ¨ / Ø§Ù„Ø­Ø¬Ø¨
    "ÙØ±Ø¶": "Ø§Ù„Ø¥Ø±Ø« Ø¨Ø§Ù„ÙØ±Ø¶", "Ø¥Ø±Ø« Ø¨Ø§Ù„ÙØ±Ø¶": "Ø§Ù„Ø¥Ø±Ø« Ø¨Ø§Ù„ÙØ±Ø¶",
    "ØªØ¹ØµÙŠØ¨": "Ø§Ù„ØªØ¹ØµÙŠØ¨", "Ø¹Ø§ØµØ¨": "Ø§Ù„ØªØ¹ØµÙŠØ¨",
    "Ø­Ø¬Ø¨": "Ø§Ù„Ø­Ø¬Ø¨", "Ù…Ø­Ø¬ÙˆØ¨": "Ø§Ù„Ø­Ø¬Ø¨",
    # Ø§Ù„ÙˆØµØ§ÙŠØ© ÙˆØ§Ù„ÙˆÙ„Ø§ÙŠØ©
    "ÙˆØµØ§ÙŠØ©": "Ø§Ù„ÙˆØµØ§ÙŠØ©", "ÙˆØµÙŠ": "Ø§Ù„ÙˆØµØ§ÙŠØ©",
    "ÙˆÙ„Ø§ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø§ØµØ±": "Ø§Ù„ÙˆÙ„Ø§ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø§ØµØ±", "Ù‚Ø§ØµØ±": "Ø§Ù„ÙˆÙ„Ø§ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø§ØµØ±",
    "Ù†Ø§Ù‚Øµ Ø£Ù‡Ù„ÙŠØ©": "Ø§Ù„ÙˆÙ„Ø§ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø§ØµØ±",
    # Ø§Ù„ØºØ§Ø¦Ø¨ ÙˆØ§Ù„Ù…ÙÙ‚ÙˆØ¯
    "Ù…ÙÙ‚ÙˆØ¯": "Ø§Ù„ØºØ§Ø¦Ø¨ ÙˆØ§Ù„Ù…ÙÙ‚ÙˆØ¯", "ØºØ§Ø¦Ø¨": "Ø§Ù„ØºØ§Ø¦Ø¨ ÙˆØ§Ù„Ù…ÙÙ‚ÙˆØ¯",

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ù†Ø¸Ø§Ù… Ø§Ù„Ø¥Ø«Ø¨Ø§Øª (22 topic)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ø¥Ù‚Ø±Ø§Ø±
    "Ø¥Ù‚Ø±Ø§Ø±": "Ø¥Ù‚Ø±Ø§Ø±", "Ø§Ø¹ØªØ±Ø§Ù": "Ø¥Ù‚Ø±Ø§Ø±", "Ù…Ù‚Ø±": "Ø¥Ù‚Ø±Ø§Ø±", "Ø£Ù‚Ø±": "Ø¥Ù‚Ø±Ø§Ø±",
    # Ø§Ø³ØªØ¬ÙˆØ§Ø¨
    "Ø§Ø³ØªØ¬ÙˆØ§Ø¨": "Ø§Ø³ØªØ¬ÙˆØ§Ø¨", "Ø§Ø³ØªØ¬ÙˆØ¨": "Ø§Ø³ØªØ¬ÙˆØ§Ø¨",
    # Ø´Ù‡Ø§Ø¯Ø© (compound first)
    "Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø´Ù‡ÙˆØ¯": "Ø´Ù‡Ø§Ø¯Ø©", "Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©": "Ø´Ù‡Ø§Ø¯Ø©", "Ù†ØµØ§Ø¨ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©": "Ø´Ù‡Ø§Ø¯Ø©",
    "Ø´Ù‡Ø§Ø¯Ø©": "Ø´Ù‡Ø§Ø¯Ø©", "Ø´Ø§Ù‡Ø¯": "Ø´Ù‡Ø§Ø¯Ø©", "Ø´Ù‡ÙˆØ¯": "Ø´Ù‡Ø§Ø¯Ø©",
    # Ø´Ø±ÙˆØ· Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©
    "Ø´Ø±ÙˆØ· Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©": "Ø´Ø±ÙˆØ· Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©", "Ø£Ù‡Ù„ÙŠØ© Ø§Ù„Ø´Ø§Ù‡Ø¯": "Ø´Ø±ÙˆØ· Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©",
    # Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©
    "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©": "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©",
    # Ø¯Ø¹ÙˆÙ‰ Ù…Ø³ØªØ¹Ø¬Ù„Ø© Ù„Ù„Ø´Ù‡Ø§Ø¯Ø©
    "Ø¯Ø¹ÙˆÙ‰ Ù…Ø³ØªØ¹Ø¬Ù„Ø© Ù„Ù„Ø´Ù‡Ø§Ø¯Ø©": "Ø¯Ø¹ÙˆÙ‰ Ù…Ø³ØªØ¹Ø¬Ù„Ø© Ù„Ù„Ø´Ù‡Ø§Ø¯Ø©",
    # ÙŠÙ…ÙŠÙ† (compound first)
    "ÙŠÙ…ÙŠÙ† Ø­Ø§Ø³Ù…Ø©": "ÙŠÙ…ÙŠÙ† Ø­Ø§Ø³Ù…Ø©", "ÙŠÙ…ÙŠÙ† Ù…ØªÙ…Ù…Ø©": "ÙŠÙ…ÙŠÙ† Ù…ØªÙ…Ù…Ø©",
    "ÙŠÙ…ÙŠÙ†": "ÙŠÙ…ÙŠÙ†", "Ø­Ù„Ù": "ÙŠÙ…ÙŠÙ†", "Ù†ÙƒÙˆÙ„": "ÙŠÙ…ÙŠÙ†", "Ø§Ø³ØªØ­Ù„Ø§Ù": "ÙŠÙ…ÙŠÙ†",
    # Ù…Ø­Ø±Ø±Ø§Øª
    "Ù…Ø­Ø±Ø±Ø§Øª Ø±Ø³Ù…ÙŠØ©": "Ù…Ø­Ø±Ø±Ø§Øª Ø±Ø³Ù…ÙŠØ©", "Ù…Ø­Ø±Ø± Ø±Ø³Ù…ÙŠ": "Ù…Ø­Ø±Ø±Ø§Øª Ø±Ø³Ù…ÙŠØ©", "Ø³Ù†Ø¯ Ø±Ø³Ù…ÙŠ": "Ù…Ø­Ø±Ø±Ø§Øª Ø±Ø³Ù…ÙŠØ©",
    "Ù…Ø­Ø±Ø±Ø§Øª Ø¹Ø§Ø¯ÙŠØ©": "Ù…Ø­Ø±Ø±Ø§Øª Ø¹Ø§Ø¯ÙŠØ©", "Ù…Ø­Ø±Ø± Ø¹Ø§Ø¯ÙŠ": "Ù…Ø­Ø±Ø±Ø§Øª Ø¹Ø§Ø¯ÙŠØ©", "Ø³Ù†Ø¯ Ø¹Ø§Ø¯ÙŠ": "Ù…Ø­Ø±Ø±Ø§Øª Ø¹Ø§Ø¯ÙŠØ©",
    "Ù…Ø­Ø±Ø±": "Ø¥Ø«Ø¨Ø§Øª Ø¨Ø§Ù„ÙƒØªØ§Ø¨Ø©", "Ù…Ø­Ø±Ø±Ø§Øª": "Ø¥Ø«Ø¨Ø§Øª Ø¨Ø§Ù„ÙƒØªØ§Ø¨Ø©", "Ù…Ø³ØªÙ†Ø¯": "Ø¥Ø«Ø¨Ø§Øª Ø¨Ø§Ù„ÙƒØªØ§Ø¨Ø©",
    "ÙƒØªØ§Ø¨Ø©": "Ø¥Ø«Ø¨Ø§Øª Ø¨Ø§Ù„ÙƒØªØ§Ø¨Ø©", "Ø³Ù†Ø¯": "Ø¥Ø«Ø¨Ø§Øª Ø¨Ø§Ù„ÙƒØªØ§Ø¨Ø©",
    # Ø¥Ù„Ø²Ø§Ù… Ø¨ØªÙ‚Ø¯ÙŠÙ… Ù…Ø­Ø±Ø±Ø§Øª
    "Ø¥Ù„Ø²Ø§Ù… Ø¨ØªÙ‚Ø¯ÙŠÙ…": "Ø¥Ù„Ø²Ø§Ù… Ø¨ØªÙ‚Ø¯ÙŠÙ… Ù…Ø­Ø±Ø±Ø§Øª", "ØªÙ‚Ø¯ÙŠÙ… Ù…Ø­Ø±Ø±Ø§Øª": "Ø¥Ù„Ø²Ø§Ù… Ø¨ØªÙ‚Ø¯ÙŠÙ… Ù…Ø­Ø±Ø±Ø§Øª",
    # ØªØ²ÙˆÙŠØ±
    "ØªØ²ÙˆÙŠØ±": "ØªØ²ÙˆÙŠØ± ÙˆØªØ­Ù‚ÙŠÙ‚ Ø®Ø·ÙˆØ·", "Ù…Ø²ÙˆØ±": "ØªØ²ÙˆÙŠØ± ÙˆØªØ­Ù‚ÙŠÙ‚ Ø®Ø·ÙˆØ·",
    "ØªØ­Ù‚ÙŠÙ‚ Ø®Ø·ÙˆØ·": "ØªØ²ÙˆÙŠØ± ÙˆØªØ­Ù‚ÙŠÙ‚ Ø®Ø·ÙˆØ·", "Ø§Ø¯Ø¹Ø§Ø¡ Ø¨Ø§Ù„ØªØ²ÙˆÙŠØ±": "ØªØ²ÙˆÙŠØ± ÙˆØªØ­Ù‚ÙŠÙ‚ Ø®Ø·ÙˆØ·",
    # Ø¯Ù„ÙŠÙ„ Ø±Ù‚Ù…ÙŠ
    "Ø¯Ù„ÙŠÙ„ Ø±Ù‚Ù…ÙŠ": "Ø¯Ù„ÙŠÙ„ Ø±Ù‚Ù…ÙŠ", "Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ": "Ø¯Ù„ÙŠÙ„ Ø±Ù‚Ù…ÙŠ", "Ø±Ù‚Ù…ÙŠ": "Ø¯Ù„ÙŠÙ„ Ø±Ù‚Ù…ÙŠ",
    "ØªÙˆÙ‚ÙŠØ¹ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ": "Ø¯Ù„ÙŠÙ„ Ø±Ù‚Ù…ÙŠ", "Ø¨Ø±ÙŠØ¯ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ": "Ø¯Ù„ÙŠÙ„ Ø±Ù‚Ù…ÙŠ",
    # Ù‚Ø±Ø§Ø¦Ù†
    "Ù‚Ø±ÙŠÙ†Ø©": "Ù‚Ø±Ø§Ø¦Ù†", "Ù‚Ø±Ø§Ø¦Ù†": "Ù‚Ø±Ø§Ø¦Ù†",
    # Ø¹Ø±Ù
    "Ø¹Ø±Ù": "Ø¹Ø±Ù", "Ø¹Ø§Ø¯Ø©": "Ø¹Ø±Ù",
    # Ù…Ø¹Ø§ÙŠÙ†Ø©
    "Ù…Ø¹Ø§ÙŠÙ†Ø©": "Ù…Ø¹Ø§ÙŠÙ†Ø©", "Ø¥Ø«Ø¨Ø§Øª Ø­Ø§Ù„Ø©": "Ù…Ø¹Ø§ÙŠÙ†Ø©", "Ø§Ù†ØªÙ‚Ø§Ù„": "Ù…Ø¹Ø§ÙŠÙ†Ø©",
    # Ø®Ø¨Ø±Ø©
    "Ø®Ø¨Ø±Ø©": "Ø®Ø¨Ø±Ø©", "Ø®Ø¨ÙŠØ±": "Ø®Ø¨Ø±Ø©", "ØªÙ‚Ø±ÙŠØ± Ø®Ø¨ÙŠØ±": "Ø®Ø¨Ø±Ø©", "Ù†Ø¯Ø¨ Ø®Ø¨ÙŠØ±": "Ø®Ø¨Ø±Ø©",
    # Ø¥Ø«Ø¨Ø§Øª Ø£Ø­ÙƒØ§Ù… Ø¹Ø§Ù…Ø©/Ø®ØªØ§Ù…ÙŠØ©
    "Ø¨ÙŠÙ†Ø©": "Ø¥Ø«Ø¨Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø¹Ø§Ù…Ø©", "Ø£Ø¯Ù„Ø©": "Ø¥Ø«Ø¨Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø¹Ø§Ù…Ø©", "Ø·Ø±Ù‚ Ø§Ù„Ø¥Ø«Ø¨Ø§Øª": "Ø¥Ø«Ø¨Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø¹Ø§Ù…Ø©",

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª Ø§Ù„Ø´Ø±Ø¹ÙŠØ© (13 topic)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ø±ÙØ¹ Ø§Ù„Ø¯Ø¹ÙˆÙ‰
    "Ø±ÙØ¹ Ø¯Ø¹ÙˆÙ‰": "Ø±ÙØ¹ Ø§Ù„Ø¯Ø¹ÙˆÙ‰", "ØµØ­ÙŠÙØ© Ø¯Ø¹ÙˆÙ‰": "Ø±ÙØ¹ Ø§Ù„Ø¯Ø¹ÙˆÙ‰", "Ù„Ø§Ø¦Ø­Ø© Ø¯Ø¹ÙˆÙ‰": "Ø±ÙØ¹ Ø§Ù„Ø¯Ø¹ÙˆÙ‰",
    "Ù‚ÙŠØ¯ Ø§Ù„Ø¯Ø¹ÙˆÙ‰": "Ø±ÙØ¹ Ø§Ù„Ø¯Ø¹ÙˆÙ‰", "Ø¯Ø¹ÙˆÙ‰": "Ø±ÙØ¹ Ø§Ù„Ø¯Ø¹ÙˆÙ‰",
    # Ø§Ø®ØªØµØ§Øµ Ø§Ù„Ù…Ø­Ø§ÙƒÙ…
    "Ø§Ø®ØªØµØ§Øµ Ù†ÙˆØ¹ÙŠ": "Ø§Ø®ØªØµØ§Øµ Ø§Ù„Ù…Ø­Ø§ÙƒÙ…", "Ø§Ø®ØªØµØ§Øµ": "Ø§Ø®ØªØµØ§Øµ Ø§Ù„Ù…Ø­Ø§ÙƒÙ…", "Ù…Ø­ÙƒÙ…Ø© Ù…Ø®ØªØµØ©": "Ø§Ø®ØªØµØ§Øµ Ø§Ù„Ù…Ø­Ø§ÙƒÙ…",
    # Ø§Ø®ØªØµØ§Øµ Ù…ÙƒØ§Ù†ÙŠ
    "Ø§Ø®ØªØµØ§Øµ Ù…ÙƒØ§Ù†ÙŠ": "Ø§Ø®ØªØµØ§Øµ Ù…ÙƒØ§Ù†ÙŠ",
    # Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø§Øª
    "Ø¬Ù„Ø³Ø©": "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø§Øª", "Ø¬Ù„Ø³Ø§Øª": "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø§Øª", "ØªØ£Ø¬ÙŠÙ„": "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø§Øª",
    # Ø­Ø¶ÙˆØ± Ø§Ù„Ø®ØµÙˆÙ…
    "Ø­Ø¶ÙˆØ±": "Ø­Ø¶ÙˆØ± Ø§Ù„Ø®ØµÙˆÙ…", "ØºÙŠØ§Ø¨": "Ø­Ø¶ÙˆØ± Ø§Ù„Ø®ØµÙˆÙ…", "Ø­ÙƒÙ… ØºÙŠØ§Ø¨ÙŠ": "Ø­Ø¶ÙˆØ± Ø§Ù„Ø®ØµÙˆÙ…",
    # Ø¯ÙÙˆØ¹ ÙˆØªØ¯Ø®Ù„
    "Ø¯ÙØ¹": "Ø¯ÙÙˆØ¹ ÙˆØªØ¯Ø®Ù„", "Ø¯ÙÙˆØ¹": "Ø¯ÙÙˆØ¹ ÙˆØªØ¯Ø®Ù„", "ØªØ¯Ø®Ù„": "Ø¯ÙÙˆØ¹ ÙˆØªØ¯Ø®Ù„",
    "Ø¥Ø¯Ø®Ø§Ù„": "Ø¯ÙÙˆØ¹ ÙˆØªØ¯Ø®Ù„", "Ø¹Ø¯Ù… Ù‚Ø¨ÙˆÙ„": "Ø¯ÙÙˆØ¹ ÙˆØªØ¯Ø®Ù„",
    # ÙˆÙ‚Ù Ø§Ù„Ø®ØµÙˆÙ…Ø©
    "ÙˆÙ‚Ù Ø§Ù„Ø®ØµÙˆÙ…Ø©": "ÙˆÙ‚Ù Ø§Ù„Ø®ØµÙˆÙ…Ø©", "ØªØ±Ùƒ Ø§Ù„Ø®ØµÙˆÙ…Ø©": "ÙˆÙ‚Ù Ø§Ù„Ø®ØµÙˆÙ…Ø©",
    # Ù‚Ø¶Ø§Ø¡ Ù…Ø³ØªØ¹Ø¬Ù„
    "Ù…Ø³ØªØ¹Ø¬Ù„": "Ù‚Ø¶Ø§Ø¡ Ù…Ø³ØªØ¹Ø¬Ù„", "Ù‚Ø¶Ø§Ø¡ Ù…Ø³ØªØ¹Ø¬Ù„": "Ù‚Ø¶Ø§Ø¡ Ù…Ø³ØªØ¹Ø¬Ù„", "Ø£Ù…Ø± Ù…Ø¤Ù‚Øª": "Ù‚Ø¶Ø§Ø¡ Ù…Ø³ØªØ¹Ø¬Ù„",
    "Ø­Ù…Ø§ÙŠØ© Ù…Ø¤Ù‚ØªØ©": "Ù‚Ø¶Ø§Ø¡ Ù…Ø³ØªØ¹Ø¬Ù„",
    # Ø¥Ù†Ù‡Ø§Ø¡Ø§Øª
    "Ø¥Ù†Ù‡Ø§Ø¡": "Ø¥Ù†Ù‡Ø§Ø¡Ø§Øª", "Ø¥Ù†Ù‡Ø§Ø¡Ø§Øª": "Ø¥Ù†Ù‡Ø§Ø¡Ø§Øª", "Ø¥Ø«Ø¨Ø§Øª ÙˆÙØ§Ø©": "Ø¥Ù†Ù‡Ø§Ø¡Ø§Øª", "Ø­ØµØ± ÙˆØ±Ø«Ø©": "Ø¥Ù†Ù‡Ø§Ø¡Ø§Øª",
    # ØªÙ†Ø­ÙŠ ÙˆØ±Ø¯Ù‘ Ø§Ù„Ù‚Ø¶Ø§Ø©
    "Ø±Ø¯ Ø§Ù„Ù‚Ø§Ø¶ÙŠ": "ØªÙ†Ø­ÙŠ ÙˆØ±Ø¯Ù‘ Ø§Ù„Ù‚Ø¶Ø§Ø©", "ØªÙ†Ø­ÙŠ": "ØªÙ†Ø­ÙŠ ÙˆØ±Ø¯Ù‘ Ø§Ù„Ù‚Ø¶Ø§Ø©", "Ø±Ø¯": "ØªÙ†Ø­ÙŠ ÙˆØ±Ø¯Ù‘ Ø§Ù„Ù‚Ø¶Ø§Ø©",
    # Ø¥Ø«Ø¨Ø§Øª ÙÙŠ Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª
    "Ø¥Ø«Ø¨Ø§Øª ÙÙŠ Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª": "Ø¥Ø«Ø¨Ø§Øª ÙÙŠ Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª",
    # Ù…Ø±Ø§ÙØ¹Ø§Øª Ø£Ø­ÙƒØ§Ù… Ø¹Ø§Ù…Ø©
    "Ù…Ø±Ø§ÙØ¹Ø§Øª": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø¹Ø§Ù…Ø©", "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ù‚Ø¶Ø§Ø¦ÙŠØ©": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø¹Ø§Ù…Ø©",
    "ØªØ¨Ù„ÙŠØº": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø¹Ø§Ù…Ø©",
    # Ø§Ø¹ØªØ±Ø§Ø¶ ÙˆØ§Ø³ØªØ¦Ù†Ø§Ù (mapped to Ø£Ø­ÙƒØ§Ù… Ø®ØªØ§Ù…ÙŠØ© since it covers appeals)
    "Ø§Ø¹ØªØ±Ø§Ø¶ Ø¹Ù„Ù‰ Ø­ÙƒÙ…": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø®ØªØ§Ù…ÙŠØ©", "Ø§Ø¹ØªØ±Ø§Ø¶": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø®ØªØ§Ù…ÙŠØ©",
    "Ø§Ø³ØªØ¦Ù†Ø§Ù": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø®ØªØ§Ù…ÙŠØ©", "Ù†Ù‚Ø¶": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø®ØªØ§Ù…ÙŠØ©",
    "Ø·Ø¹Ù†": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø®ØªØ§Ù…ÙŠØ©", "Ø§Ù„ØªÙ…Ø§Ø³ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù†Ø¸Ø±": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø®ØªØ§Ù…ÙŠØ©",
    "ØªÙ…ÙŠÙŠØ²": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø®ØªØ§Ù…ÙŠØ©",
    # ØªÙ†ÙÙŠØ°
    "ØªÙ†ÙÙŠØ° Ø­ÙƒÙ…": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø®ØªØ§Ù…ÙŠØ©", "ØªÙ†ÙÙŠØ°": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø®ØªØ§Ù…ÙŠØ©",
}


def _enrich_followup(question: str, chat_history: list | None) -> str:
    """Enrich a follow-up question with context from chat history.

    When a user asks a short follow-up like "Ù…Ù† Ù‡ÙŠ Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø°Ø§Øª Ø§Ù„Ø§Ø®ØªØµØ§ØµØŸ"
    after discussing custody/divorce, we combine the original topic with the
    new question so the RAG pipeline finds the right articles.
    """
    if not chat_history:
        return question

    q = question.strip()

    # Heuristic: short questions (< 40 chars) with no legal keywords
    # are likely follow-ups that need context enrichment
    detected = _detect_topics(q)
    if detected and len(detected) >= 2:
        return question  # Already has enough context

    # Extract topic keywords from recent user messages in chat history
    topic_keywords = []
    for msg in reversed(chat_history[-4:]):
        if msg.get("role") == "user":
            prev_topics = _detect_topics(msg.get("content", ""))
            for t in prev_topics:
                if t not in topic_keywords:
                    topic_keywords.append(t)
            if topic_keywords:
                break  # Use the most recent user message's topics

    if not topic_keywords:
        return question

    # Build enriched query: original question + topic context
    topic_str = " ".join(topic_keywords[:2])
    enriched = f"{question} ({topic_str})"
    print(f"ðŸ”— Enriched follow-up: '{question}' â†’ '{enriched}'")
    return enriched


def retrieve_context(question: str, top_k: int = 5, chat_history: list | None = None) -> dict:
    """
    Hybrid retrieval: semantic search + keyword-based topic filtering.
    Merges topic-matched results (high precision) with semantic results (recall).
    """
    # Enrich follow-up questions with context from chat history
    enriched_question = _enrich_followup(question, chat_history)

    cache_key = enriched_question.strip()
    if cache_key in _rag_cache:
        return _rag_cache[cache_key]

    classification = classify_query(question)  # Classify original question
    query_embedding = embed_query_list(enriched_question)  # Search with enriched

    # === 1. Broad semantic search (for recall) ===
    semantic_results = search(query_embedding, n_results=top_k * 2)

    # === 2. Keyword-based topic search (for precision) ===
    detected_topics = _detect_topics(enriched_question)
    filtered_results = None

    if detected_topics:
        for topic in detected_topics[:2]:
            where_filter = {"topic": {"$eq": topic}}
            filtered_results = search(query_embedding, n_results=top_k, where=where_filter)
            if filtered_results["documents"] and filtered_results["documents"][0]:
                break

    # === 3. Merge: topic-matched first (precise), then semantic (broad) ===
    merged = _merge_results(semantic_results, filtered_results, top_k)

    context = build_context_string(merged, classification)

    result = {
        "classification": classification,
        "context": context,
        "sources": extract_sources(merged),
        "num_results": len(merged["documents"][0]) if merged["documents"] else 0,
    }

    if len(_rag_cache) >= _RAG_CACHE_MAX:
        oldest_key = next(iter(_rag_cache))
        del _rag_cache[oldest_key]
    _rag_cache[cache_key] = result

    return result


def _normalize_arabic(text: str) -> str:
    """Light Arabic normalization for better matching."""
    # Remove common prefixes/suffixes that block substring matching
    # Also strip Ø§Ù„ Ø§Ù„ØªØ¹Ø±ÙŠÙ from question for flexible matching
    import re
    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)  # Normalize alef variants
    return text


# Verb/derived forms â†’ topic mapping (handles Arabic morphology)
LEGAL_VERB_MAP = {
    "Ø£Ø¹ØªØ±Ø¶": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø®ØªØ§Ù…ÙŠØ©", "ÙŠØ¹ØªØ±Ø¶": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø®ØªØ§Ù…ÙŠØ©",
    "Ø£Ø³ØªØ£Ù†Ù": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø®ØªØ§Ù…ÙŠØ©", "ÙŠØ³ØªØ£Ù†Ù": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø®ØªØ§Ù…ÙŠØ©",
    "Ø£Ø·Ø¹Ù†": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø®ØªØ§Ù…ÙŠØ©", "ÙŠØ·Ø¹Ù†": "Ù…Ø±Ø§ÙØ¹Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø®ØªØ§Ù…ÙŠØ©",
    "Ø£Ø±ÙØ¹": "Ø±ÙØ¹ Ø§Ù„Ø¯Ø¹ÙˆÙ‰", "ÙŠØ±ÙØ¹": "Ø±ÙØ¹ Ø§Ù„Ø¯Ø¹ÙˆÙ‰",
    "Ø£ÙˆØ«Ù‚": "Ø¹Ù‚Ø¯ Ø§Ù„Ø²ÙˆØ§Ø¬", "ÙŠÙˆØ«Ù‚": "Ø¹Ù‚Ø¯ Ø§Ù„Ø²ÙˆØ§Ø¬",
    "Ø£Ø«Ø¨Øª": "Ø¥Ø«Ø¨Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø¹Ø§Ù…Ø©", "ÙŠØ«Ø¨Øª": "Ø¥Ø«Ø¨Ø§Øª - Ø£Ø­ÙƒØ§Ù… Ø¹Ø§Ù…Ø©",
    "Ø£Ù†ÙÙ‚": "Ø§Ù„Ù†ÙÙ‚Ø©", "ÙŠÙ†ÙÙ‚": "Ø§Ù„Ù†ÙÙ‚Ø©",
    "Ø·Ù„Ù‚Ù†ÙŠ": "Ø§Ù„Ø·Ù„Ø§Ù‚", "Ø·Ù„Ù‚Ù‡Ø§": "Ø§Ù„Ø·Ù„Ø§Ù‚", "ÙŠØ·Ù„Ù‚": "Ø§Ù„Ø·Ù„Ø§Ù‚",
    "Ø®Ø§Ù„Ø¹Øª": "Ø§Ù„Ø®Ù„Ø¹", "Ø®Ø§Ù„Ø¹Ù†ÙŠ": "Ø§Ù„Ø®Ù„Ø¹",
}

# Short words that are too ambiguous â€” only match as whole words
_SHORT_AMBIGUOUS = {"Ø¹Ø±Ù", "Ø±Ø¯", "Ø³Ù†Ø¯", "Ø¯ÙØ¹", "Ø±ÙƒÙ†", "Ø£Ù‚Ø±"}


def _detect_topics(question: str) -> list[str]:
    """Detect specific legal topics from question keywords (longest match first).
    Uses both LEGAL_TERM_MAP (noun phrases) and LEGAL_VERB_MAP (verb forms).
    """
    topics = []
    seen = set()
    q = question.strip()

    # 1. Check verb forms first
    for verb, topic in LEGAL_VERB_MAP.items():
        if verb in q and topic not in seen:
            topics.append(topic)
            seen.add(topic)

    # 2. Sort LEGAL_TERM_MAP by key length descending for longest-match-first
    sorted_terms = sorted(LEGAL_TERM_MAP.items(), key=lambda x: len(x[0]), reverse=True)

    for term, topic in sorted_terms:
        if topic in seen:
            continue

        # Short ambiguous words: require word-boundary match (space or start/end)
        if term in _SHORT_AMBIGUOUS:
            import re
            if re.search(r'(?:^|\s)' + re.escape(term) + r'(?:\s|$)', q):
                topics.append(topic)
                seen.add(topic)
            continue

        # For regular terms: also try with/without Ø§Ù„ prefix for flexibility
        if term in q:
            topics.append(topic)
            seen.add(topic)
        elif len(term) > 3:
            # Try adding/removing Ø§Ù„ for flexible matching
            if term.startswith("Ø§Ù„") and term[2:] in q:
                topics.append(topic)
                seen.add(topic)
            elif not term.startswith("Ø§Ù„") and ("Ø§Ù„" + term) in q:
                topics.append(topic)
                seen.add(topic)

    return topics


def _merge_results(semantic: dict, filtered: dict | None, top_k: int) -> dict:
    """Merge filtered (high precision) + semantic (broad recall), deduplicated."""
    if not filtered or not filtered["documents"] or not filtered["documents"][0]:
        return _trim(semantic, top_k)

    seen = set()
    docs, metas, dists = [], [], []

    # Filtered results first (they match the legal topic)
    for doc, meta, dist in zip(
        filtered["documents"][0], filtered["metadatas"][0], filtered["distances"][0],
    ):
        key = doc[:100]
        if key not in seen:
            seen.add(key)
            docs.append(doc)
            metas.append(meta)
            dists.append(dist)

    # Then semantic results (for additional context)
    if semantic["documents"] and semantic["documents"][0]:
        for doc, meta, dist in zip(
            semantic["documents"][0], semantic["metadatas"][0], semantic["distances"][0],
        ):
            key = doc[:100]
            if key not in seen:
                seen.add(key)
                docs.append(doc)
                metas.append(meta)
                dists.append(dist)

    return {
        "documents": [docs[:top_k]],
        "metadatas": [metas[:top_k]],
        "distances": [dists[:top_k]],
    }


def _trim(results: dict, top_k: int) -> dict:
    if not results["documents"] or not results["documents"][0]:
        return results
    return {
        "documents": [results["documents"][0][:top_k]],
        "metadatas": [results["metadatas"][0][:top_k]],
        "distances": [results["distances"][0][:top_k]],
    }


def build_context_string(results: dict, classification: dict) -> str:
    """Build a formatted context string from search results."""
    parts = []
    if not results["documents"] or not results["documents"][0]:
        parts.append("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙˆØ§Ø¯ Ø°Ø§Øª ØµÙ„Ø©.")
        return "\n".join(parts)

    for i, (doc, meta, dist) in enumerate(zip(
        results["documents"][0], results["metadatas"][0], results["distances"][0],
    )):
        law_name = meta.get("law", "Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ©")
        section = meta.get("section", "")
        parts.append(f"[{i+1}] {law_name} | {section}" if section else f"[{i+1}] {law_name}")
        parts.append(doc)
        if meta.get("has_deadline") == "True":
            parts.append(f"â° Ù…Ù‡Ù„Ø©: {meta.get('deadline_details', '')}")
        parts.append("")

    return "\n".join(parts)


def extract_sources(results: dict) -> list[dict]:
    """Extract source references from results."""
    sources = []
    if not results["metadatas"] or not results["metadatas"][0]:
        return sources
    for meta in results["metadatas"][0]:
        sources.append({
            "chapter": meta.get("chapter", ""),
            "section": meta.get("section", ""),
            "topic": meta.get("topic", ""),
            "pages": meta.get("source_pages", ""),
        })
    return sources
