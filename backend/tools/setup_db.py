"""
Initialize the vector database with law articles.

Loads pre-computed embeddings from backend/data/embeddings.json
(generated by precompute_gemini.py). No API calls needed.

If embeddings.json is not available, falls back to Gemini API.
"""
import json
import os
import sys

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.dirname(project_root))

from backend.config import ARTICLES_JSON_PATH, CHROMA_PERSIST_DIR
from backend.rag.vector_store import add_documents, get_collection_count

EMBEDDINGS_PATH = os.path.join(os.path.dirname(ARTICLES_JSON_PATH), "embeddings.json")


def setup_database(force_rebuild: bool = False):
    """Load articles and pre-computed embeddings into ChromaDB.

    Supports resuming: if the DB already has some articles,
    only the missing ones are added.
    """
    print("=" * 60)
    print("إعداد قاعدة البيانات المتجهة (Vector Database)")
    print("=" * 60)

    # Load articles
    if not os.path.exists(ARTICLES_JSON_PATH):
        print(f"خطأ: ملف المواد غير موجود: {ARTICLES_JSON_PATH}")
        return

    with open(ARTICLES_JSON_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)

    articles = data["articles"]
    total = len(articles)
    print(f"تم تحميل {total} مقطع")

    # Check existing state
    existing_count = get_collection_count()

    if force_rebuild and existing_count > 0:
        print(f"إعادة بناء كاملة — حذف {existing_count} مقطع موجود...")
        import chromadb
        client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIR)
        client.delete_collection("saudi_family_law")
        import backend.rag.vector_store as vs
        vs._collection = None
        vs._client = None
        existing_count = 0

    if existing_count >= total:
        print(f"✅ قاعدة البيانات مكتملة — {existing_count}/{total} مقطع")
        return

    # Find which article IDs are already stored
    existing_ids = set()
    if existing_count > 0:
        from backend.rag.vector_store import get_collection
        col = get_collection()
        stored = col.get(include=[])
        existing_ids = set(stored["ids"])
        print(f"تم العثور على {len(existing_ids)}/{total} مقطع محفوظ — استكمال الباقي...")

    # Filter to only missing articles
    missing = [a for a in articles if a["id"] not in existing_ids]
    print(f"المقاطع المتبقية: {len(missing)}")

    if not missing:
        print("✅ جميع المقاطع محفوظة بالفعل")
        return

    # Load pre-computed embeddings
    precomputed = {}
    if os.path.exists(EMBEDDINGS_PATH):
        print(f"تحميل التضمينات المحسوبة مسبقاً من {EMBEDDINGS_PATH}...")
        with open(EMBEDDINGS_PATH, "r") as f:
            precomputed = json.load(f)
        print(f"  تم تحميل {len(precomputed)} تضمين")

    # Separate articles into those with pre-computed embeddings and those without
    has_embedding = [a for a in missing if a["id"] in precomputed]
    no_embedding = [a for a in missing if a["id"] not in precomputed]

    print(f"  مع تضمين محسوب: {len(has_embedding)}")
    print(f"  بدون تضمين: {len(no_embedding)}")

    # Store articles with pre-computed embeddings (fast, no API)
    if has_embedding:
        print(f"\nتخزين {len(has_embedding)} مقطع بتضمينات محسوبة مسبقاً...")
        batch_size = 100
        for i in range(0, len(has_embedding), batch_size):
            batch = has_embedding[i:i+batch_size]
            ids = [a["id"] for a in batch]
            texts = [a["text"] for a in batch]
            embeddings = [precomputed[a["id"]] for a in batch]
            metadatas = [_make_metadata(a) for a in batch]
            add_documents(ids, texts, embeddings, metadatas)
            print(f"  ✓ {min(i+batch_size, len(has_embedding))}/{len(has_embedding)}")

    # Store articles without pre-computed embeddings (needs API)
    if no_embedding:
        print(f"\nتوليد تضمينات لـ {len(no_embedding)} مقطع عبر API...")
        try:
            from backend.rag.embeddings import embed_texts
        except Exception as e:
            print(f"⚠️ لا يمكن استيراد embed_texts: {e}")
            print(f"  تخطي {len(no_embedding)} مقطع — أضف embeddings.json مكتمل")
            no_embedding = []

        batch_size = 25
        for i in range(0, len(no_embedding), batch_size):
            batch = no_embedding[i:i+batch_size]
            ids = [a["id"] for a in batch]
            texts = [a["text"] for a in batch]
            metadatas = [_make_metadata(a) for a in batch]
            try:
                embeddings = embed_texts(texts)
                add_documents(ids, texts, embeddings, metadatas)
                print(f"  ✓ {min(i+batch_size, len(no_embedding))}/{len(no_embedding)} (API)")
            except Exception as e:
                print(f"  ⚠️ فشل: {e}")
                continue

    final_count = get_collection_count()
    print(f"\n✓ إجمالي المقاطع في قاعدة البيانات: {final_count}/{total}")
    if final_count >= total:
        print("✅ قاعدة البيانات مكتملة!")
    elif final_count > 0:
        print(f"  ⚠️ {total - final_count} مقطع ناقص")


def _make_metadata(article: dict) -> dict:
    """Create ChromaDB metadata for an article."""
    # Ensure all values are non-empty strings (ChromaDB rejects empty lists)
    source_pages = article.get("source_pages", "")
    if isinstance(source_pages, list):
        source_pages = ", ".join(str(p) for p in source_pages) if source_pages else ""
    topic_tags = article.get("topic_tags", [])
    if isinstance(topic_tags, list):
        topic_tags = ",".join(str(t) for t in topic_tags) if topic_tags else ""
    return {
        "chapter": article.get("chapter", "") or "",
        "section": article.get("section", "") or "",
        "topic": article.get("topic", "") or "",
        "topic_tags": topic_tags,
        "has_deadline": str(article.get("has_deadline", False)),
        "deadline_details": article.get("deadline_details", "") or "",
        "source_pages": source_pages,
        "law": article.get("law", "نظام الأحوال الشخصية"),
    }


if __name__ == "__main__":
    setup_database()
